# Document Organizer Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# AI Provider Configuration
# =============================================================================

# Select which AI provider to use (auto-detects if not set)
# Options: anthropic, claude-code, openai, bedrock, ollama, keywords
AI_PROVIDER=

# -----------------------------------------------------------------------------
# Anthropic Claude
# -----------------------------------------------------------------------------
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# Model to use (default: claude-sonnet-4-20250514)
# Options: claude-sonnet-4-20250514, claude-3-5-sonnet-20241022, claude-3-haiku-20240307
ANTHROPIC_MODEL=

# -----------------------------------------------------------------------------
# OpenAI
# -----------------------------------------------------------------------------
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Model to use (default: gpt-4o)
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
OPENAI_MODEL=

# -----------------------------------------------------------------------------
# AWS Bedrock
# -----------------------------------------------------------------------------
# Uses AWS credentials from environment or ~/.aws/credentials
# No API key needed - uses IAM authentication

# AWS region for Bedrock (default: us-east-1)
AWS_REGION=

# AWS profile name (optional, uses default if not set)
AWS_PROFILE=

# Bedrock model (default: claude-3-sonnet)
# Options: claude-3-sonnet, claude-3-haiku, claude-3-opus, titan-express, titan-lite
BEDROCK_MODEL=

# -----------------------------------------------------------------------------
# Ollama (Local)
# -----------------------------------------------------------------------------
# No API key needed - runs locally

# Ollama server URL (default: http://localhost:11434)
OLLAMA_URL=

# Model to use (default: llama3.2)
# Options: llama3.2, mistral, mixtral, codellama, etc.
OLLAMA_MODEL=

# =============================================================================
# Directory Configuration
# =============================================================================
# These paths are used for LOCAL Python execution.
# For Docker, use DOCUMENTS_PATH instead (see Docker section below).

# Output directory - root of your Johnny.Decimal folder structure
# Default: ~/Documents/jd_documents
OUTPUT_DIR=

# Input directory - where new documents are placed for processing
# Default: {OUTPUT_DIR}/00-09 System/01 Inbox
INBOX_DIR=

# =============================================================================
# Docker Configuration
# =============================================================================
# When running with Docker, set DOCUMENTS_PATH to mount your folder.
# The container maps this to /documents internally.
#
# Example: DOCUMENTS_PATH=/Users/yourname/Documents
# Then in the app, use paths like /documents/jd_documents

DOCUMENTS_PATH=

# =============================================================================
# OCR Configuration
# =============================================================================

# Path to Tesseract OCR binary (if not in system PATH)
# macOS (Homebrew): /opt/homebrew/bin/tesseract
# Linux: /usr/bin/tesseract
# Windows: C:\Program Files\Tesseract-OCR\tesseract.exe
TESSERACT_CMD=

# =============================================================================
# Logging Configuration
# =============================================================================

# Log level (default: INFO)
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file path (leave empty for stdout only)
LOG_FILE=
